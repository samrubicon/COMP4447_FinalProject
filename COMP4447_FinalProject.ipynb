{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import urllib.robotparser\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "import time\n",
    "import dateparser\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GeckoDriverManager module.\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "\n",
    "# Install the GeckoDriverManager to run FireFox web browser (for the first time!)\n",
    "# driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "\n",
    "# Once driver is installed, use this line:\n",
    "driver = webdriver.Firefox(executable_path='./geckodriver.exe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandcamp Web Scraper Data format information\n",
    "\n",
    "'''\n",
    "\n",
    "scrape_data = pd.DataFrame(columns=['Release Title', \n",
    "                                    'Artist Name', \n",
    "                                    'Artist Location',\n",
    "                                    'Release Date',\n",
    "                                    'Release URL',\n",
    "                                    'Release Genre',\n",
    "                                    'Release Sub-Genre',\n",
    "                                    'Search Format',\n",
    "                                    'Search Week',\n",
    "                                    'Search Category',\n",
    "                                    'All Lyrics',\n",
    "                                    'Number of Tracks'\n",
    "                                    'Track Info',\n",
    "                                    'Number of Fans',\n",
    "                                    'Tags',\n",
    "                                    'Scrape Date'])\n",
    "                                    \n",
    "### dictionary entry for track listing\n",
    "\n",
    "track_info_entry = {\n",
    "    'Track Title' : '',\n",
    "    'Track Lyrics' : '',\n",
    "    'Track Number' : '#',\n",
    "    'Track Duration': '##:##'\n",
    "}\n",
    "                                    \n",
    "'''\n",
    "\n",
    "# valid weeks: [-1,0,678,677,676,675,674,673]\n",
    "# today, this week, last week, 2, 3, 4, 5, 6 weeks ago\n",
    "WEEK_DICT = {\n",
    "    -1:'today',\n",
    "    0:'this week',\n",
    "    678:'last week',\n",
    "    677:'2 weeks ago',\n",
    "    676:'3 weeks ago',\n",
    "    675:'4 weeks ago',\n",
    "    674:'5 weeks ago',\n",
    "    673:'6 weeks ago'\n",
    "}\n",
    "\n",
    "# scrape dataframe list to collect release entries\n",
    "scrape_data = []\n",
    "\n",
    "# all URLS collected so far (don't collect duplicate albums for multiple searches)\n",
    "# load 'all_URLS_collected.txt' file if it exists\n",
    "all_URLS_collected = []\n",
    "URLS_file = Path(\"all_URLS_collected.txt\")\n",
    "if URLS_file.is_file():\n",
    "    with open(\"all_URLS_collected.txt\", \"r\") as fp:\n",
    "        all_URLS_collected = json.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape bandcamp release page function\n",
    "def scrape_release_page(URL,entry,output=False):\n",
    "    \n",
    "    entry = entry.copy()\n",
    "    \n",
    "    entry['Release URL'] = URL\n",
    "    entry['Scrape Date'] = datetime.datetime.now().date()\n",
    "    \n",
    "    # don't add entry if it already exists in scrape dataframe\n",
    "    if URL in all_URLS_collected:\n",
    "        return\n",
    "    else:\n",
    "        all_URLS_collected.append(URL)\n",
    "    \n",
    "    if output==True:\n",
    "        print('retrieving info for ' + URL + ' ...')\n",
    "\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(URL,headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "\n",
    "        soup = bsoup(response.text, 'lxml')\n",
    "        \n",
    "        # Get title and artist name\n",
    "        title = soup.find('meta',{'name':'title'})['content'].split(', by ')\n",
    "        entry['Release Title'] = title[0]\n",
    "        entry['Artist Name'] = title[1]\n",
    "        \n",
    "        # Get artist location\n",
    "        entry['Artist Location'] = soup.find('span',{'class':'location secondaryText'}).string\n",
    "        \n",
    "        # Get release date\n",
    "        release_date = soup.find('div',{'class':'tralbumData tralbum-credits'}).contents[0].string.strip('\"').strip(' ').replace('releases','').replace('released','')\n",
    "        release_date = dateparser.parse(release_date)\n",
    "        entry['Release Date'] = release_date\n",
    "        \n",
    "        # Get tags\n",
    "        tags = soup.find_all('a',{'class':'tag'})\n",
    "        tags_list = []\n",
    "        for tag in tags:\n",
    "            tags_list.append(tag.string)\n",
    "        entry['Tags'] = tags_list\n",
    "        \n",
    "        # If no genre in entry, leave blank\n",
    "        if 'Release Genre' not in entry:\n",
    "            entry['Release Genre'] = 'N/A'\n",
    "            \n",
    "        # If no subgenre in entry, leave blank\n",
    "        if 'Release Sub-Genre' not in entry:\n",
    "            entry['Release Sub-Genre'] = 'N/A'            \n",
    "        \n",
    "        # Get track information\n",
    "        entry['Track Info'] = []\n",
    "        #tracks = soup.find('table',{'id':'track_table'}).findChildren('tr', recursive=False)\n",
    "        tracks = soup.find('table',{'id':'track_table'}).find_all('tr',{'class':'track_row_view'})\n",
    "        \n",
    "        all_lyrics = ''\n",
    "        for track in tracks:\n",
    "            # Track number\n",
    "            track_num = int(track.find('td',{'class':'track-number-col'}).div.string.strip('.'))\n",
    "\n",
    "            # Track title\n",
    "            track_title = track.find('span',{'class':'track-title'})\n",
    "            if track_title:\n",
    "                track_title = track_title.string\n",
    "            else:\n",
    "                track_title = track.find('div',{'class':'title'}).span.string\n",
    "        \n",
    "            # Track duration\n",
    "            track_duration = track.find('span',{'class':'time secondaryText'})\n",
    "            if track_duration:\n",
    "                track_duration = track_duration.string.replace('\\n','').strip(' ')\n",
    "            else:\n",
    "                track_duration = 'N/A'  \n",
    "                \n",
    "            # Track lyrics\n",
    "            track_lyrics = 'N/A'\n",
    "            \n",
    "            track_lyric_link = track.find('div',{'class':'info_link'}).a['href']            \n",
    "            if track_lyric_link and '#lyrics' in track_lyric_link:\n",
    "                lyric_tag = track.findNext('tr',{'class':'lyricsRow'})\n",
    "                track_lyrics = lyric_tag.find('td',{'colspan':'4'}).div.string.strip('\"').replace('\\r\\n','\\n')\n",
    "                all_lyrics += '\\n' + track_lyrics\n",
    "            \n",
    "            # Add track object to tracks list\n",
    "            track_obj = {'Track Title': track_title, 'Track Lyrics': track_lyrics,\n",
    "                        'Track Number': track_num, 'Track Duration': track_duration}\n",
    "            entry['Track Info'].append(track_obj)\n",
    "\n",
    "        if all_lyrics != '':\n",
    "            entry['All Lyrics'] = all_lyrics\n",
    "        else:\n",
    "            entry['All Lyrics'] = 'N/A'\n",
    "        \n",
    "        # Number of Tracks\n",
    "        entry['Number of Tracks'] = len(entry['Track Info'])\n",
    "        \n",
    "        # Popularity Index\n",
    "        entry['Number of Fans'] = 'N/A'\n",
    "        driver.get(URL)\n",
    "        foundAllFans = False\n",
    "        fan_pages_searched = 0\n",
    "        while foundAllFans == False and fan_pages_searched < MAX_FAN_PAGES:\n",
    "            try:\n",
    "                more_thumbs = driver.find_element_by_xpath('//a[@class=\"more-thumbs\"]')\n",
    "                fan_pages_searched += 1\n",
    "                if 'display: none' in more_thumbs.get_attribute(\"style\"):\n",
    "                    foundAllFans = True\n",
    "            except:\n",
    "                foundAllFans = True\n",
    "        \n",
    "        if fan_pages_searched == MAX_FAN_PAGES:\n",
    "            entry['Number of Fans'] = '>' + str(MAX_FANS)\n",
    "        else:\n",
    "            try:\n",
    "                parentElement = driver.find_element_by_xpath('//div[@class=\"no-writing\"]')\n",
    "                elementList = parentElement.find_elements_by_tag_name(\"a\")\n",
    "                entry['Number of Fans'] = len(elementList)                   \n",
    "            except:\n",
    "                entry['Number of Fans'] = 0\n",
    "                \n",
    "        if output == True:\n",
    "            #pprint(entry)\n",
    "            #print()\n",
    "            pass\n",
    "        \n",
    "        # add entry to scrape data\n",
    "        scrape_data.append(entry)\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape search page function\n",
    "# scrape the number of pages in the given search URL\n",
    "def scrape_search_page(URL,num_pages=NUM_PAGES,output=False):\n",
    "\n",
    "    # read parameters from link\n",
    "    parameters = URL.split('?')[1:][0].split('&')\n",
    "    genre = 'all'\n",
    "    search_category = 'top'\n",
    "    location = 0\n",
    "    formatt = 'all'\n",
    "    subgenre = 'all'\n",
    "    week = 0\n",
    "    for parameter in parameters:\n",
    "        p = parameter.split('=')\n",
    "        if p[0] == 'g':\n",
    "            genre = p[1]\n",
    "        if p[0] == 's':\n",
    "            search_category = p[1]\n",
    "        if p[0] == 'gn':\n",
    "            location = int(p[1])\n",
    "        if p[0] == 'f':\n",
    "            formatt = p[1]\n",
    "        if p[0] == 't':\n",
    "            subgenre = p[1]\n",
    "        if p[0] == 'w':\n",
    "            week = int(p[1])\n",
    "    \n",
    "    print('scraping ' + category + ' ' + genre + ' (' + subgenre + ') albums from location \\'' + str(location) + '\\' in ' + formatt + ' format for week ' + str(week) + '...')\n",
    "    \n",
    "    entry = {'Release Genre': genre,\n",
    "            'Release Sub-Genre': subgenre,\n",
    "            'Search Format': formatt,\n",
    "            'Search Week': WEEK_DICT[week],\n",
    "            'Search Category': search_category}\n",
    "    \n",
    "    driver.get(URL)\n",
    "    time.sleep(INITIAL_SEARCH_WAIT)\n",
    "    \n",
    "    # extract URLs from link tags\n",
    "    release_URLS = []\n",
    "    \n",
    "    for i in range(num_pages):\n",
    "        # this is just to ensure that the page is loaded\n",
    "        \n",
    "        html = driver.page_source\n",
    "\n",
    "        # create bsoup object\n",
    "        soup = bsoup(str(html), 'lxml')\n",
    "\n",
    "        # find album link tags on this page\n",
    "        link_tags = soup.find_all('a', {'class':'item-title'})\n",
    "    \n",
    "        for tag in link_tags:\n",
    "            tag_url = tag['href'].split('?')[0]\n",
    "            if tag_url not in release_URLS and tag_url not in all_URLS_collected:\n",
    "                release_URLS.append(tag_url)\n",
    "        \n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//a[contains(text(), 'next')]\").click()\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    if output == True:\n",
    "        pprint(release_URLS)\n",
    "\n",
    "    for release_URL in release_URLS:\n",
    "        scrape_release_page(release_URL,entry,output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### USER INPUTS ######################################################\n",
    "\n",
    "# list of genres to scrape\n",
    "GENRES = ['all',\n",
    "          'rock',\n",
    "          #'metal',\n",
    "          'alternative',\n",
    "          'hip-hop-rap',\n",
    "          'experimental',\n",
    "          #'punk',\n",
    "          'pop',\n",
    "          'acoustic',\n",
    "          #'funk',\n",
    "          'country',\n",
    "          'blues',\n",
    "          #'ambient',\n",
    "          #'soundtrack',\n",
    "          #'world',\n",
    "          'jazz',\n",
    "          'r-b-soul',\n",
    "          #'devotional',\n",
    "          #'classical',\n",
    "          'reggae'\n",
    "          #'latin'\n",
    "         ]\n",
    "\n",
    "# scrape for subgenres within genres?\n",
    "SCRAPE_SUBGENRES = False\n",
    "\n",
    "# dictionary of subgenres to scrape, if scrape_subgenres = True\n",
    "SUBGENRES = {\n",
    "                'rock': ['indie','prog-rock','post-rock','rock-roll','psychedelic-rock'],\n",
    "                'metal': [],\n",
    "                'alternative': [],\n",
    "                'hip-hop-rap': [],\n",
    "                'experimental': [],\n",
    "                'punk': [],\n",
    "                'folk': [],\n",
    "                'pop': [],\n",
    "                'acoustic': [],\n",
    "                'funk': [],\n",
    "                'country': [],\n",
    "                'blues': []\n",
    "            }\n",
    "\n",
    "# list of search categories to scrape ['top', 'new', 'rec']\n",
    "#   top = best-selling\n",
    "#   new = new arrivals\n",
    "#   rec = artist-recommended\n",
    "SEARCH_CATEGORIES = ['top','new']\n",
    "\n",
    "# list of locations to scrape\n",
    "# location = 0 returns search results for all locations\n",
    "LOCATIONS = [0] \n",
    "\n",
    "# list of formats to scrape ['all','digital','vinyl','cd','cassette']\n",
    "FORMATS = ['all']\n",
    "\n",
    "# list of weeks to scrape\n",
    "# valid weeks: [-1,0,678,677,676,675,674,673]\n",
    "# today, this week, last week, 2, 3, 4, 5, 6 weeks ago\n",
    "WEEKS = [673]\n",
    "\n",
    "# include week parameter in search?\n",
    "SCRAPE_WEEKS = True\n",
    "\n",
    "# number of pages to scrape (10 produces 40-100 results)\n",
    "NUM_PAGES = 13\n",
    "\n",
    "# Initial time to wait (in seconds) for Bandcamp Discover page to load\n",
    "INITIAL_SEARCH_WAIT = 5\n",
    "\n",
    "# retrieve no more than this many fans for popularity index\n",
    "MAX_FANS = 100000\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "# max number of pages of fan results to scrape before hitting MAX_FANS\n",
    "MAX_FAN_PAGES = int(MAX_FANS / 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping top all (all) albums from location '0' in all format for week 673...\n"
     ]
    }
   ],
   "source": [
    "# main web scraper loop\n",
    "\n",
    "for genre in GENRES:\n",
    "    for category in SEARCH_CATEGORIES:\n",
    "        for location in LOCATIONS:\n",
    "            for formatt in FORMATS:\n",
    "                for week in WEEKS:\n",
    "                    if SCRAPE_WEEKS:\n",
    "                        scrape_URL = 'https://bandcamp.com/?g=' + genre + '&s=' + category + '&p=0' + '&gn=' + str(location) + '&f=' + formatt + '&w=' + str(week)\n",
    "                    else:\n",
    "                        scrape_URL = 'https://bandcamp.com/?g=' + genre + '&s=' + category + '&p=0' + '&gn=' + str(location) + '&f=' + formatt\n",
    "                        \n",
    "                    scrape_search_page(scrape_URL,output=False)\n",
    "                    \n",
    "                    if SCRAPE_SUBGENRES and genre in SUBGENRES:\n",
    "                        for subgenre in SUBGENRES[genre]:\n",
    "                            subgenre_scrape_URL = scrape_URL + '&t=' + subgenre\n",
    "                            scrape_search_page(subgenre_scrape_URL,output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOHN KING CAVE scrapes\n",
    "\n",
    "#scrape_release_page('https://johnkingcave.bandcamp.com/album/devil-rides-beside',{'Release Genre':'folk'})\n",
    "#scrape_release_page('https://johnkingcave.bandcamp.com/album/oh-my-love',{'Release Genre':'folk'})\n",
    "#scrape_release_page('https://johnkingcave.bandcamp.com/album/emotion-tread-light',{'Release Genre':'folk'})\n",
    "#scrape_release_page('https://johnkingcave.bandcamp.com/album/720-split',{'Release Genre':'folk'})\n",
    "#scrape_release_page('https://johnkingcave.bandcamp.com/album/sing-a-song',{'Release Genre':'folk'})\n",
    "\n",
    "# NOTE: Doesn't work for Bandcamp track pages\n",
    "#scrape_release_page('https://johnkingcave.bandcamp.com/track/i-love-america',{'Release Genre':'folk'})\n",
    "#scrape_release_page('https://johnkingcave.bandcamp.com/track/shes-coming-down-the-line-2',{'Release Genre':'folk'})\n",
    "#scrape_release_page('https://johnkingcave.bandcamp.com/track/labyrinth-of-faith',{'Release Genre':'folk'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Dataframe\n",
    "scrape_df = pd.DataFrame(scrape_data)\n",
    "scrape_df = scrape_df.drop_duplicates(subset = [\"Release URL\"])\n",
    "\n",
    "scrape_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe to CSV\n",
    "\n",
    "timestamp = time.strftime('%b-%d-%Y_%H%M%S')\n",
    "scrape_df.to_csv('scrape_data_' + timestamp + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save collected URLS (to be loaded for next batch)\n",
    "with open(\"all_URLS_collected.txt\", \"w\") as fp:\n",
    "    json.dump(all_URLS_collected, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
